
# listenIn: Enhancing Video Accessibility for the Visually Impaired

**Overview:**

listenIn is a project aimed at improving accessibility to video content for visually impaired individuals through advanced captioning and audio description technologies. By leveraging AI models for image captioning, text-to-speech conversion, and synchronization techniques, our goal is to provide a comprehensive and inclusive viewing experience.

**Key Features:**

- **Image Captioning:** Utilizes the BLIP model to generate detailed captions synchronized with video frames.
  
- **Text-to-Speech:** Converts text descriptions into natural-sounding audio narrations using gTTS.
  
- **Speech-to-Text:** Transcribes video audio tracks to facilitate accurate synchronization of captions.
  
- **Synchronization:** Ensures precise alignment between visual and auditory elements for seamless playback.

**Challenges and Future Directions:**

Our challenges include enhancing caption accuracy across diverse video types and improving the alignment of captions with speech. Future efforts will focus on expanding monitoring and testing to ensure robust performance under varied conditions.

**Dependencies:**

- Python
- OpenCV
- Transformers (Hugging Face)
- gTTS (Google Text-to-Speech)
- Whisper (OpenAI)
- MoviePy
- FFmpeg

**Contributing:**

Contributions are welcome! Please fork the repository, make your changes, and submit a pull request. For major changes, please open an issue first to discuss potential updates.

**Special Notice:**

If the name of the input video contains parenthesis or other special characters, it is likely to generate an error.

**Contact:**

For questions or feedback, contact us at maxizi226487@gmail.com
